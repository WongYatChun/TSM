{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## How LambdaMART works\n\nHere is the high-level workflow of xgboost LambdaMart:\n\n### 1. **Initial Model and Baseline Prediction**\n- The process begins with an initial model that makes baseline predictions about the relevance of items. This model can be very simple, often just predicting the average relevance score.\n\n### 2. **Calculation of Gradients (Lambdas)**\n- After the initial predictions, the algorithm calculates gradients, which, in ranking tasks, are represented by lambda values. These lambdas are derived from the difference in a ranking metric (like NDCG) that would result from swapping pairs of items in the ranking. They quantify the direction and magnitude of the error in the ranking.\n\n### 3. **Building Weak Learners to Predict Gradients**\n- Gradient boosting then builds weak learners (typically decision trees) to predict these gradients rather than directly predicting the relevance scores. Each tree focuses on correcting the errors made by the ensemble of previously built trees.\n\n### 4. **Adjusting Predictions**\n- The predictions from the model are adjusted based on the output of these weak learners, effectively moving the predicted ranking closer to the optimal ranking. The adjustments are made in a direction that improves the overall ranking metric, with the magnitude of adjustment modulated by the learning rate.\n\n### 5. **Iterative Improvement**\n- This process of calculating gradients, building trees to predict these gradients, and adjusting the model's predictions is repeated iteratively. With each iteration, the model becomes better at predicting the correct ranking order of items.\n\n### 6. **Ensemble of Weak Learners**\n- The final model is an ensemble of all the weak learners built throughout the iterations. This ensemble model is capable of accurately predicting the relevance of items, resulting in a ranking that maximizes the chosen ranking metric.\n\n","metadata":{}},{"cell_type":"markdown","source":"## How lambdas are calculated\n\nTo provide a more concrete example with step-by-step numerical calculations for the LambdaMART algorithm using NDCG as the metric, let's visit a simplified scenario with 3 items: A, B, and C. We'll include the actual formulas for NDCG and show how lambda values are calculated and used to adjust the model's scores.\n\n### Simplified Scenario Setup\n- **Items and their true relevance scores**: A=3, B=2, C=1\n- **Initial Model Scores** leading to the ranking: C > B > A (an incorrect ranking based on scores, not relevance)\n\n### Objective\nOptimize the ranking to match true relevance: **Ideal Ranking**: A > B > C\n\n### Step 1: Calculate Initial NDCG\nNDCG is calculated using the formula:\n$$ NDCG@k = \\frac{DCG@k}{IDCG@k} $$\n\nwhere\n$$ DCG@k = \\sum_{i=1}^{k} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)} $$\n$ IDCG@k $ is the DCG score of the ideal ranking.\n\nFor simplicity, let's consider NDCG@3 for all items.\n\n#### Initial DCG\nGiven the initial ranking (C > B > A), let's calculate DCG@3:\n$$ DCG@3 = \\frac{2^1 - 1}{\\log_2(2)} + \\frac{2^2 - 1}{\\log_2(3)} + \\frac{2^3 - 1}{\\log_2(4)} $$\n\n#### Ideal DCG (IDCG)\nFor the ideal ranking (A > B > C):\n$$ IDCG@3 = \\frac{2^3 - 1}{\\log_2(2)} + \\frac{2^2 - 1}{\\log_2(3)} + \\frac{2^1 - 1}{\\log_2(4)} $$\n\nLet's calculate these values.\n\nWith the calculations done:\n\n- **Initial DCG** (for the incorrect ranking C > B > A): 6.39\n- **IDCG** (for the ideal ranking A > B > C): 9.39\n- **Initial NDCG**: 0.68\n\nThis NDCG value indicates the effectiveness of the initial ranking compared to the ideal ranking.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef dcg(relevances):\n    \"\"\"Calculate Discounted Cumulative Gain (DCG).\"\"\"\n    relevances = np.array(relevances)\n    # We use np.arange to create a denominator that starts from 2 to len(relevances) + 1\n    # because the log base 2 of 1 is 0, and we are trying to avoid DivisionByZero error.\n    denominators = np.log2(np.arange(2, len(relevances) + 2))\n    return np.sum((2**relevances - 1) / denominators)\n\ndef ndcg(predictions, relevances):\n    \"\"\"Calculate Normalized Discounted Cumulative Gain (NDCG).\"\"\"\n    # Convert predictions to numpy array and sort them according to the predicted scores\n    # Assuming higher scores should rank higher, we sort both predictions and relevances\n    # based on predictions to simulate the ranking process.\n    predictions = np.array(predictions)\n    relevances = np.array(relevances)\n    predicted_order = np.argsort(predictions)[::-1]\n    sorted_relevances = relevances[predicted_order]\n    \n    # Calculate DCG for the predicted ranking\n    predicted_dcg = dcg(sorted_relevances)\n    \n    # Calculate DCG for the ideal ranking\n    ideal_relevances = np.sort(relevances)[::-1]\n    ideal_dcg = dcg(ideal_relevances)\n    \n    # Avoid division by zero\n    if ideal_dcg == 0:\n        return 0\n    \n    # Calculate NDCG\n    return predicted_dcg / ideal_dcg\n\n# Example usage:\nrelevances = [3, 2, 1]  # True relevance scores\npredictions = [1, 2, 3]  # Predicted relevance scores (model output)\n\nndcg_score = ndcg(predictions, relevances)\nprint(f\"NDCG Score: {ndcg_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-28T16:28:25.552297Z","iopub.execute_input":"2024-02-28T16:28:25.552694Z","iopub.status.idle":"2024-02-28T16:28:25.563193Z","shell.execute_reply.started":"2024-02-28T16:28:25.552665Z","shell.execute_reply":"2024-02-28T16:28:25.561844Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"NDCG Score: 0.6806060567602009\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 2: Calculate Lambda Values\nLambda values would be calculated based on the impact of swapping pairs on the NDCG. However, for simplicity, let's illustrate how lambda values might be adjusted conceptually, given the substantial difference between the initial and ideal NDCG values.","metadata":{}},{"cell_type":"code","source":"def calculate_loss_changes_with_labels(items, preds, values):\n    \"\"\"\n    Calculate and document the changes in the loss function for all pairwise swaps, using item labels.\n\n    Parameters:\n    - items: List of item labels (e.g., [\"A\", \"B\", \"C\"]).\n    - values: List of corresponding values for the items.\n\n    Returns:\n    - A list of dictionaries, each containing details of the swap (using labels) and the change in loss.\n    \"\"\"\n    # Initial loss calculation\n    initial_loss = ndcg(preds, values)\n    \n    def compute_loss(preds, values):\n        \"\"\"Helper function to compute loss for any given order of values.\"\"\"\n        return ndcg(preds, values)\n\n    swaps_and_changes = []\n\n    for i in range(len(items)):\n        for j in range(i + 1, len(items)):\n            # Swap the values corresponding to the items\n            new_preds = preds.copy()\n            new_preds[i], new_preds[j] = new_preds[j], new_preds[i]\n            \n            new_loss = compute_loss(new_preds, values)\n            change = new_loss - initial_loss\n            \n            # Use item labels for documenting the swap\n            swap_details = {\n                'swap': f\"{items[i]} <-> {items[j]}\",\n                'change': change\n            }\n            swaps_and_changes.append(swap_details)\n\n    return swaps_and_changes\n\n# Items and their values\nitems_example = [\"A\", \"B\", \"C\"]\npreds_example = [1, 2, 3]\nvalues_example = [3, 2, 1]\n\n# Calculate and document swaps\nresult_with_labels = calculate_loss_changes_with_labels(items_example, preds_example, values_example)\n\n# Example output\nfor r in result_with_labels:\n    print(f\"Swap: {r['swap']}, Change in Loss Function: {r['change']}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T16:18:14.154320Z","iopub.execute_input":"2024-02-28T16:18:14.154709Z","iopub.status.idle":"2024-02-28T16:18:14.166400Z","shell.execute_reply.started":"2024-02-28T16:18:14.154681Z","shell.execute_reply":"2024-02-28T16:18:14.165142Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Swap: A <-> B, Change in Loss Function: 0.05575756037413726\nSwap: A <-> C, Change in Loss Function: 0.3193939432397991\nSwap: B <-> C, Change in Loss Function: 0.07858586755953101\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Simplified Aggregation Approach:**\n\n- Lambda for A: Sum of changes from swaps involving A (A ↔ B, A ↔ C).\n- Lambda for B: Sum of changes from swaps involving B (A ↔ B, B ↔ C) but considering the direction of the effect.\n- Lambda for C: Sum of changes from swaps involving C (A ↔ C, B ↔ C), again considering the direction.\n\nLet's proceed with this simplified aggregation:\n\nThe aggregate lambda values, based on the simplified aggregation of NDCG changes due to swaps, are as follows:\n\n- Lambda for A: +0.375\n- Lambda for B: +0.023\n- Lambda for C: -0.39\n\n\n**Interpretation of Aggregate Lambda Values**\n\n- A: The positive lambda value indicates a strong need to increase A's model score to move it towards its correct higher position in the ranking, reflecting its high relevance.\n\n- B: The smaller positive lambda value for B suggests a moderate adjustment is needed, fine-tuning its position likely closer to its current one but ensuring it's correctly ordered relative to A and C.\n\n- C: The negative lambda value for C indicates a need to decrease C's model score, correcting its initial over-ranking by moving it towards a lower position in the ranking.","metadata":{}}]}